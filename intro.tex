\chapter{Introduction}
Neural Networks have gained great popularity both in academia as well as industry due to their proven ability to solve difficult tasks with reassuring capability. They are already being employed as part of conversational agents, autonomous vehicles and facial recognition software in the industry, just to name a few applications. Likewise, they have also shown great promise for sensitive operations such as facial recognition, speech recognition, precision medicine and other fields. In fields like malware analysis and network anomaly detection, there is a growing drive to share learned models within the community, which can help in serving quicker and more standardized responses to such issues. 

However, it is important to draw the distinction between sharing hard-coded rules, such as antivirus policies and signatures, and learned anomaly classifiers. While the former can be easily interpreted and understood, the latter is often treated as a black box when attempting to derive the logic it serves. Additionally, the logic for the latter is a function derived from the training data it sees. As a simple example, consider the case of Support Vector Machines, a linear model often used for classification or regression. The classifier learned by the SVM, a hyperplane which divides the domain space into 2, can be represented as a weighted sum of it's training data points. Hence, while sharing classifiers, one must keep in mind that there is inevitably some leakage of data, and consequently privacy, as well.

Property Inference is the problem of inferring properties about the training dataset using only the parameters of the trained classifier as prior. These properties could range from bias in the dataset to existence of certain types of samples detec